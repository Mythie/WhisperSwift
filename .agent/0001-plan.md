# WhisperSwift Implementation Plan

## Overview

WhisperSwift is an idiomatic Swift binding library for whisper.cpp, designed for production use with a focus on real-time audio transcription. The library wraps the whisper.cpp C API with modern Swift patterns including async/await, actors for thread safety, and AsyncSequence for streaming results.

## Requirements Summary

| Requirement | Decision |
|-------------|----------|
| Platform | macOS (iOS later) |
| Primary Use Case | Real-time microphone transcription via AVAudioEngine |
| Secondary Use Case | Batch file transcription |
| Streaming Granularity | Segment-by-segment with VAD |
| Session Model | Single continuous session |
| Model Loading | User-specified path |
| GPU Acceleration | Metal + CoreML support |
| XCFramework | Committed to repository |
| Minimum macOS | 13.3 |
| Error Handling | Stop stream on error with detailed reporting |

## Architecture

### Package Structure

```
WhisperSwift/
├── .agent/
│   └── 0001-plan.md                 # This file
│
├── Sources/
│   └── WhisperSwift/
│       ├── WhisperSwift.swift       # Public API exports
│       │
│       ├── Core/
│       │   ├── WhisperContext.swift # Actor wrapping whisper_context
│       │   ├── VADContext.swift     # Actor wrapping Silero VAD (optional)
│       │   └── WhisperError.swift   # Error types
│       │
│       ├── Configuration/
│       │   ├── WhisperConfiguration.swift  # Hardware options
│       │   ├── TranscriptionOptions.swift  # Transcription params
│       │   ├── VADOptions.swift            # Silero VAD options (optional)
│       │   └── Language.swift              # Supported languages
│       │
│       ├── Transcription/
│       │   ├── Transcriber.swift           # Batch transcription API
│       │   ├── StreamingTranscriber.swift  # Real-time streaming API
│       │   ├── StreamingState.swift        # Streaming state enum
│       │   └── TranscriptionResult.swift   # Result types
│       │
│       └── Audio/
│           ├── AudioProcessor.swift        # Format conversion utilities
│           ├── AudioRingBuffer.swift       # Thread-safe ring buffer
│           └── SilenceDetector.swift       # Lightweight RMS-based silence detection
│
├── Tests/
│   └── WhisperSwiftTests/
│       ├── WhisperSwiftTests.swift         # Unit tests (74 tests)
│       ├── IntegrationTests.swift          # Integration tests with model
│       └── Fixtures/                       # Test assets
│
├── whisper.cpp/                     # Git submodule
│   └── build-apple/
│       └── whisper.xcframework/     # Pre-built framework (generated)
│
└── Package.swift
```

### Build Integration

We will use the pre-built XCFramework approach:

1. Run `whisper.cpp/build-xcframework.sh` to generate the framework
2. Reference it as a `.binaryTarget` in Package.swift
3. The framework includes Metal and CoreML support with embedded shaders

**Package.swift structure:**

```swift
// swift-tools-version: 6.0
import PackageDescription

let package = Package(
    name: "WhisperSwift",
    platforms: [
        .macOS(.v13)
    ],
    products: [
        .library(name: "WhisperSwift", targets: ["WhisperSwift"])
    ],
    targets: [
        .target(
            name: "WhisperSwift",
            dependencies: ["whisper"],
            swiftSettings: [
                .swiftLanguageMode(.v6)
            ]
        ),
        .binaryTarget(
            name: "whisper",
            path: "whisper.cpp/build-apple/whisper.xcframework"
        ),
        .testTarget(
            name: "WhisperSwiftTests",
            dependencies: ["WhisperSwift"]
        )
    ]
)
```

---

## Public API Design

### Core Types

#### `Transcriber` - Batch Transcription

```swift
/// A transcriber for converting audio files to text.
///
/// Use this for batch processing of audio files. For real-time microphone
/// transcription, use ``StreamingTranscriber`` instead.
///
/// ```swift
/// let transcriber = try await Transcriber(modelPath: modelURL)
/// let result = try await transcriber.transcribe(file: audioURL)
/// print(result.text)
/// ```
public final class Transcriber: Sendable {
    
    /// Creates a transcriber with the specified model.
    /// - Parameters:
    ///   - modelPath: Path to the whisper.cpp GGML model file
    ///   - configuration: Hardware and processing configuration
    /// - Throws: `WhisperError.modelLoadFailed` if the model cannot be loaded
    public init(
        modelPath: URL,
        configuration: WhisperConfiguration = .default
    ) async throws
    
    /// Transcribes an audio file.
    /// - Parameters:
    ///   - file: URL to the audio file (WAV, MP3, M4A, etc.)
    ///   - options: Transcription options
    /// - Returns: The transcription result with text and segments
    /// - Throws: `WhisperError` on failure
    public func transcribe(
        file: URL,
        options: TranscriptionOptions = .default
    ) async throws -> TranscriptionResult
    
    /// Transcribes raw audio samples.
    /// - Parameters:
    ///   - samples: Audio samples as Float32 values (-1.0 to 1.0), must be 16kHz mono
    ///   - options: Transcription options
    /// - Returns: The transcription result with text and segments
    public func transcribe(
        samples: [Float],
        options: TranscriptionOptions = .default
    ) async throws -> TranscriptionResult
}
```

#### `StreamingTranscriber` - Real-Time Transcription

```swift
/// A transcriber for real-time audio streaming with Voice Activity Detection.
///
/// This is the primary API for real-time microphone transcription. It uses VAD
/// to detect speech segments and emits transcription results as an AsyncSequence.
///
/// ```swift
/// let transcriber = try await StreamingTranscriber(modelPath: modelURL)
/// 
/// // Start processing and iterate over segments
/// try await transcriber.start()
/// 
/// for try await segment in transcriber.segments {
///     print("[\(segment.startTime)]: \(segment.text)")
/// }
/// ```
///
/// Feed audio samples from AVAudioEngine:
/// ```swift
/// audioEngine.inputNode.installTap(onBus: 0, bufferSize: 4096, format: format) { buffer, time in
///     Task {
///         try await transcriber.process(buffer: buffer)
///     }
/// }
/// ```
public final class StreamingTranscriber: Sendable {
    
    /// Creates a streaming transcriber with the specified model.
    /// - Parameters:
    ///   - modelPath: Path to the whisper.cpp GGML model file
    ///   - configuration: Hardware and processing configuration
    public init(
        modelPath: URL,
        configuration: WhisperConfiguration = .default
    ) async throws
    
    /// Starts the streaming session.
    ///
    /// Must be called before processing audio. The transcriber will begin
    /// accepting audio samples and emitting segments.
    public func start() async throws
    
    /// Processes an audio buffer from AVAudioEngine.
    ///
    /// The buffer will be automatically converted to the required format
    /// (16kHz mono Float32). Call this method from your AVAudioEngine tap.
    ///
    /// - Parameter buffer: Audio buffer from AVAudioEngine
    /// - Throws: `WhisperError.notStarted` if `start()` hasn't been called
    public func process(buffer: AVAudioPCMBuffer) async throws
    
    /// Processes raw audio samples.
    ///
    /// - Parameters:
    ///   - samples: Audio samples as Float32 values (-1.0 to 1.0)
    ///   - sampleRate: Sample rate of the input audio
    public func process(samples: [Float], sampleRate: Double) async throws
    
    /// An async sequence of transcribed segments.
    ///
    /// Iterate over this sequence to receive transcription results in real-time.
    /// The sequence completes when `stop()` is called or an error occurs.
    public var segments: AsyncThrowingStream<TranscriptionSegment, Error> { get }
    
    /// Stops the streaming session and finalizes transcription.
    ///
    /// Any remaining audio in the buffer will be processed before stopping.
    /// After calling this, `segments` will complete.
    ///
    /// - Returns: The complete transcription result
    public func stop() async throws -> TranscriptionResult
    
    /// The current state of the transcriber.
    public var state: StreamingState { get }
}

/// The state of a streaming transcriber.
public enum StreamingState: Sendable {
    case idle
    case running
    case stopping
    case stopped
    case failed(WhisperError)
}
```

### Configuration Types

```swift
/// Hardware and processing configuration for whisper.cpp.
public struct WhisperConfiguration: Sendable {
    /// Whether to use GPU acceleration (Metal on macOS).
    ///
    /// Automatically disabled on iOS simulator. On real devices,
    /// this provides significant performance improvements.
    public var useGPU: Bool
    
    /// Whether to use flash attention for better Metal performance.
    ///
    /// Recommended when `useGPU` is true on Apple Silicon.
    public var useFlashAttention: Bool
    
    /// Number of CPU threads to use.
    ///
    /// If `nil`, automatically determined based on processor count.
    /// When using GPU, fewer CPU threads are typically needed.
    public var threadCount: Int?
    
    /// Default configuration with GPU acceleration enabled.
    public static let `default`: WhisperConfiguration
    
    /// CPU-only configuration (useful for testing or fallback).
    public static let cpuOnly: WhisperConfiguration
}

/// Options for transcription behavior.
public struct TranscriptionOptions: Sendable {
    /// The language of the audio.
    ///
    /// Set to `nil` or `.auto` for automatic language detection.
    public var language: Language?
    
    /// Whether to translate to English instead of transcribing.
    public var translate: Bool
    
    /// Whether to include token-level timestamps.
    public var tokenTimestamps: Bool
    
    /// Initial prompt to condition the model.
    ///
    /// Can be used to provide context or guide transcription style.
    public var initialPrompt: String?
    
    /// Sampling strategy for decoding.
    public var samplingStrategy: SamplingStrategy
    
    /// Voice Activity Detection configuration.
    public var vad: VADOptions?
    
    /// Default options for general transcription.
    public static let `default`: TranscriptionOptions
}

/// Sampling strategy for the decoder.
public enum SamplingStrategy: Sendable {
    /// Greedy decoding - fastest, good for most cases.
    case greedy
    
    /// Beam search - slower but potentially more accurate.
    case beamSearch(beamSize: Int)
}

/// Voice Activity Detection options.
public struct VADOptions: Sendable {
    /// Probability threshold to consider audio as speech (0.0-1.0).
    public var threshold: Float
    
    /// Minimum duration for a valid speech segment in milliseconds.
    public var minSpeechDurationMs: Int
    
    /// Minimum silence duration to consider speech ended in milliseconds.
    public var minSilenceDurationMs: Int
    
    /// Padding added before and after detected speech in milliseconds.
    public var speechPadMs: Int
    
    /// Default VAD options.
    public static let `default`: VADOptions
}
```

### Result Types

```swift
/// The result of a transcription operation.
public struct TranscriptionResult: Sendable {
    /// The complete transcribed text.
    public let text: String
    
    /// Individual segments with timing information.
    public let segments: [TranscriptionSegment]
    
    /// The detected language, if auto-detection was used.
    public let detectedLanguage: Language?
    
    /// Performance timing information.
    public let timings: TranscriptionTimings?
}

/// A single segment of transcribed text with timing.
public struct TranscriptionSegment: Sendable, Identifiable {
    /// Unique identifier for this segment.
    public let id: Int
    
    /// The transcribed text for this segment.
    public let text: String
    
    /// Start time in seconds from the beginning of audio.
    public let startTime: TimeInterval
    
    /// End time in seconds from the beginning of audio.
    public let endTime: TimeInterval
    
    /// Individual tokens with their probabilities (if requested).
    public let tokens: [Token]?
    
    /// Probability that this segment contains no speech.
    public let noSpeechProbability: Float
    
    /// Whether this segment indicates a speaker turn.
    public let isSpeakerTurn: Bool
}

/// A single token from the transcription.
public struct Token: Sendable, Identifiable {
    public let id: Int
    public let text: String
    public let probability: Float
    public let startTime: TimeInterval?
    public let endTime: TimeInterval?
}

/// Performance timing information.
public struct TranscriptionTimings: Sendable {
    public let sampleMs: Float
    public let encodeMs: Float
    public let decodeMs: Float
    public let totalMs: Float
}
```

### Error Types

```swift
/// Errors that can occur during whisper.cpp operations.
public enum WhisperError: Error, Sendable {
    /// The model file was not found at the specified path.
    case modelNotFound(URL)
    
    /// The model failed to load (invalid format, corrupted, etc.).
    case modelLoadFailed(String)
    
    /// The audio file could not be read or decoded.
    case audioLoadFailed(URL, underlying: Error)
    
    /// The audio format is not supported or couldn't be converted.
    case invalidAudioFormat(String)
    
    /// Transcription failed during processing.
    case transcriptionFailed(String)
    
    /// The streaming transcriber is not in the correct state.
    case invalidState(expected: StreamingState, actual: StreamingState)
    
    /// VAD model failed to load or process.
    case vadFailed(String)
    
    /// An internal whisper.cpp error occurred.
    case internalError(String)
}
```

### Language Enumeration

```swift
/// Languages supported by Whisper models.
public enum Language: String, CaseIterable, Sendable {
    case english = "en"
    case chinese = "zh"
    case german = "de"
    case spanish = "es"
    case russian = "ru"
    case korean = "ko"
    case french = "fr"
    case japanese = "ja"
    case portuguese = "pt"
    case turkish = "tr"
    case polish = "pl"
    case catalan = "ca"
    case dutch = "nl"
    case arabic = "ar"
    case swedish = "sv"
    case italian = "it"
    case indonesian = "id"
    case hindi = "hi"
    case finnish = "fi"
    case vietnamese = "vi"
    case hebrew = "he"
    case ukrainian = "uk"
    case greek = "el"
    case malay = "ms"
    case czech = "cs"
    case romanian = "ro"
    case danish = "da"
    case hungarian = "hu"
    case tamil = "ta"
    case norwegian = "no"
    case thai = "th"
    case urdu = "ur"
    case croatian = "hr"
    case bulgarian = "bg"
    case lithuanian = "lt"
    case latin = "la"
    case maori = "mi"
    case malayalam = "ml"
    case welsh = "cy"
    case slovak = "sk"
    case telugu = "te"
    case persian = "fa"
    case latvian = "lv"
    case bengali = "bn"
    case serbian = "sr"
    case azerbaijani = "az"
    case slovenian = "sl"
    case kannada = "kn"
    case estonian = "et"
    case macedonian = "mk"
    case breton = "br"
    case basque = "eu"
    case icelandic = "is"
    case armenian = "hy"
    case nepali = "ne"
    case mongolian = "mn"
    case bosnian = "bs"
    case kazakh = "kk"
    case albanian = "sq"
    case swahili = "sw"
    case galician = "gl"
    case marathi = "mr"
    case punjabi = "pa"
    case sinhala = "si"
    case khmer = "km"
    case shona = "sn"
    case yoruba = "yo"
    case somali = "so"
    case afrikaans = "af"
    case occitan = "oc"
    case georgian = "ka"
    case belarusian = "be"
    case tajik = "tg"
    case sindhi = "sd"
    case gujarati = "gu"
    case amharic = "am"
    case yiddish = "yi"
    case lao = "lo"
    case uzbek = "uz"
    case faroese = "fo"
    case haitianCreole = "ht"
    case pashto = "ps"
    case turkmen = "tk"
    case nynorsk = "nn"
    case maltese = "mt"
    case sanskrit = "sa"
    case luxembourgish = "lb"
    case myanmar = "my"
    case tibetan = "bo"
    case tagalog = "tl"
    case malagasy = "mg"
    case assamese = "as"
    case tatar = "tt"
    case hawaiian = "haw"
    case lingala = "ln"
    case hausa = "ha"
    case bashkir = "ba"
    case javanese = "jw"
    case sundanese = "su"
    case cantonese = "yue"
    
    /// Automatic language detection.
    case auto = "auto"
    
    /// The full English name of the language.
    public var displayName: String { ... }
}
```

---

## Internal Implementation

### WhisperContext Actor

The core wrapper around the C API, ensuring thread safety:

```swift
/// Internal actor wrapping whisper_context for thread-safe access.
actor WhisperContext {
    private let context: OpaquePointer
    
    init(modelPath: String, params: whisper_context_params) throws {
        guard let ctx = whisper_init_from_file_with_params(modelPath, params) else {
            throw WhisperError.modelLoadFailed("Failed to initialize context")
        }
        self.context = ctx
    }
    
    deinit {
        whisper_free(context)
    }
    
    func transcribe(samples: [Float], params: whisper_full_params) throws -> [RawSegment] {
        let result = samples.withUnsafeBufferPointer { ptr in
            whisper_full(context, params, ptr.baseAddress, Int32(samples.count))
        }
        guard result == 0 else {
            throw WhisperError.transcriptionFailed("whisper_full returned \(result)")
        }
        return extractSegments()
    }
    
    private func extractSegments() -> [RawSegment] {
        let count = whisper_full_n_segments(context)
        return (0..<count).map { i in
            RawSegment(
                text: String(cString: whisper_full_get_segment_text(context, i)),
                t0: whisper_full_get_segment_t0(context, i),
                t1: whisper_full_get_segment_t1(context, i),
                noSpeechProb: whisper_full_get_segment_no_speech_prob(context, i)
            )
        }
    }
}
```

### Audio Processing

Audio conversion utilities for handling various input formats:

```swift
/// Converts AVAudioPCMBuffer to the format required by whisper.cpp.
struct AudioProcessor {
    /// Target sample rate for whisper.cpp
    static let targetSampleRate: Double = 16000
    
    /// Converts an AVAudioPCMBuffer to Float32 samples at 16kHz mono.
    static func convert(_ buffer: AVAudioPCMBuffer) throws -> [Float] {
        // 1. Convert to mono if stereo
        // 2. Resample to 16kHz if needed
        // 3. Convert to Float32 if needed
        // 4. Normalize to -1.0...1.0 range
    }
    
    /// Loads and converts an audio file to Float32 samples.
    static func loadAudioFile(_ url: URL) throws -> [Float] {
        let file = try AVAudioFile(forReading: url)
        // Read and convert
    }
}
```

### Streaming Implementation

The streaming transcriber manages a ring buffer and VAD:

```swift
actor StreamingTranscriberState {
    private var audioBuffer: [Float] = []
    private var processedSamples: Int = 0
    private let context: WhisperContext
    private var vadContext: VADContext?
    
    func appendSamples(_ samples: [Float]) {
        audioBuffer.append(contentsOf: samples)
    }
    
    func processIfReady(options: TranscriptionOptions) async throws -> TranscriptionSegment? {
        // Check VAD for speech detection
        // If speech segment complete, transcribe and return
        // Otherwise return nil
    }
}
```

---

## Implementation Phases

### Phase 1: Foundation (Complete)
- [x] Create implementation plan
- [x] Build whisper.xcframework for macOS
- [x] Set up Package.swift with binary target
- [x] Create basic project structure
- [x] Implement WhisperContext actor
- [x] Basic error types

### Phase 2: Batch Transcription (Complete)
- [x] Implement Transcriber class
- [x] Audio file loading with AVFoundation
- [x] Audio format conversion (resampling, mono conversion)
- [x] TranscriptionResult and TranscriptionSegment types
- [x] Basic tests with sample audio (25 tests passing)

### Phase 3: Real-Time Streaming (Complete)
- [x] Implement StreamingTranscriber
- [x] Ring buffer for audio accumulation (AudioRingBuffer actor)
- [x] AsyncThrowingStream for segment delivery
- [x] Integration with AVAudioEngine patterns (process(buffer:) method)
- [x] VAD integration for segment detection (VADContext actor, VADOptions)
- [x] StreamingState enum for state management
- [x] Lightweight silence detection (SilenceDetector) as default chunking strategy
- [x] Unit tests for streaming components (74 total tests)

### Phase 4: Configuration & Polish (Complete)
- [x] Full WhisperConfiguration options
- [x] TranscriptionOptions (language, translate, etc.)
- [x] VADOptions configuration
- [x] Language enum with all 99 languages
- [x] Comprehensive error messages

### Phase 5: Testing & Documentation
- [x] Unit tests for all public APIs (74 tests passing)
- [x] Integration tests with real models
- [x] Performance benchmarks
- [ ] DocC documentation
- [ ] Example usage in README

---

## Technical Considerations

### Thread Safety

whisper.cpp's `whisper_context` is **not thread-safe**. We use Swift actors to ensure:
- Only one transcription runs at a time per context
- State mutations are serialized
- Safe use with Swift concurrency

### Memory Management

- Models can be large (40MB - 1.5GB)
- Audio buffers grow during streaming
- Use `autoreleasepool` for Objective-C bridging in tight loops
- Consider memory pressure handling for iOS (later)

### Audio Format Requirements

whisper.cpp requires:
- **Sample rate**: 16000 Hz
- **Channels**: 1 (mono)
- **Format**: Float32 (-1.0 to 1.0)

AVAudioEngine typically provides 48kHz stereo, so conversion is always needed.

### VAD Integration

Voice Activity Detection has two modes:

1. **Lightweight Silence Detection (Default)**: Uses `SilenceDetector` for RMS-based 
   silence gap detection. No external model required. Good for most use cases.
   - Configurable via `SilenceDetectorOptions`
   - Uses Accelerate framework for efficient RMS calculation
   - Searches for natural silence gaps to split audio chunks

2. **Neural VAD (Optional)**: Uses Silero VAD via `whisper_vad_*` functions.
   - Requires user to provide Silero VAD model path
   - More accurate for challenging audio conditions
   - Configurable via `VADOptions`

The `StreamingTranscriber` automatically uses silence detection when no VAD model
is provided, making it easy to get started without additional dependencies.

### Performance Targets

For real-time transcription on Apple Silicon:
- Encoding: < 500ms for 30s audio chunk
- Decoding: Real-time factor < 0.5x (faster than real-time)
- Memory: < 500MB for base model with streaming

---

## Dependencies

### Required Frameworks (linked by xcframework)
- Foundation
- Accelerate (BLAS operations)
- Metal (GPU acceleration)
- CoreML (encoder acceleration, iOS/macOS only)

### Build Dependencies
- Xcode 15+ with Swift 6
- CMake 3.28+ (for building xcframework)
- macOS 13.3+ SDK

---

## Open Questions

1. **VAD Model**: Should we bundle the Silero VAD model, or require users to provide it?
   - **Decision**: Neural VAD is optional; lightweight silence detection is the default.
     Users can provide a Silero VAD model path for more accurate detection if needed.

2. **Logging**: Should we expose whisper.cpp's internal logging?
   - **Decision**: Yes, via a configurable log handler.

3. **Progress Callbacks**: Should batch transcription report progress?
   - **Decision**: Yes, via an optional progress handler parameter.

---

## References

- [whisper.cpp GitHub](https://github.com/ggerganov/whisper.cpp)
- [whisper.cpp API Header](whisper.cpp/include/whisper.h)
- [Apple AVAudioEngine Documentation](https://developer.apple.com/documentation/avfaudio/avaudioengine)
- [Swift Concurrency](https://docs.swift.org/swift-book/LanguageGuide/Concurrency.html)
